from transformers import CLIPModel, CLIPProcessor
import torch
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
dummy_image = torch.randn(1, 3, 224, 224)
dummy_text = processor(["sample text"], return_tensors="pt")["input_ids"]
torch.onnx.export(model, (dummy_image, dummy_text), "models/clip/1/model.onnx", opset_version=12)